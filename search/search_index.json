{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KNN SKLEARN, K-NEAREST NEIGHBOR MENGGUNAKAN SCIKIT LEARN DENGAN BREAST CANCER DATA SET \u00b6 Implementasi knn classifier di scikit belajar Dalam pengantar k tetangga terdekat dan penerapan pengklasifikasi knn di Python dari awal, Kami membahas aspek-aspek kunci dari algoritma knn dan mengimplementasikan algoritma knn dengan cara yang mudah untuk beberapa dataset pengamatan. Namun dalam implementasiK-Neighbor di scikit learn, kita akan memeriksa Dataset Cancer Breast menggunakan library python sklearn untuk memodelkan algoritma k-nn. Setelah memodelkan knn classifier, kita akan menggunakan model knn tranining untuk memprediksi apakah pasien menderita tumor jinak atau tumor ganas. Kehebatan menggunakan Sklearn adalah memberikan kita fungsionalitas untuk mengimplementasikan algoritma machine learning dalam beberapa baris kode. Ketika kita membahas prinsip di balik algoritma KNN classifier (K-Nearest Neighbor) adalah untuk menemukan jumlah training sampel K yang telah ditentukan terdekat dalam jarak ke titik baru & memprediksi label dari ini. Ukuran jarak umumnya dianggap sebagai jarak Euclidean. Euclidean Distance adalah ukuran jarak yang paling umum digunakan. Jarak Euclidean juga disebut sebagai jarak semata. Penggunaan ukuran jarak Euclidean sangat dianjurkan saat data padat atau kontinu. Jarak Euclidean adalah ukuran kedekatan terbaik. Jarak Euclidean antara dua titik adalah panjang jalur yang menghubungkannya. Teorema Pythagoras memberikan jarak ini antara dua titik. Implementasi knn dengan Sklearn \u00b6 Wisconsin Breast Cancer Dataset \u00b6 Wisconsin Breast Cancer Dataset dikumpulkan oleh Dr. William H. Wolberg (dokter), Rumah Sakit Universitas Wisconsin, AS. Dataset ini terdiri dari 10 atribut kontinu dan 1 atribut kelas target. Atribut Class menunjukkan hasil pengamatan, apakah pasien menderita tumor jinak atau tumor ganas. Tumor jinak tidak menyebar ke bagian lain sementara tumor ganas bersifat kanker. Dataset dikumpulkan & didistribusikan secara terbuka untuk mengetahui beberapa pola dari data ini. Atribut Class menunjukkan hasil pengamatan, apakah pasien menderita tumor jinak atau tumor ganas. Tumor jinak tidak menyebar ke bagian lain sementara tumor ganas bersifat kanker. Dataset dikumpulkan & didistribusikan secara terbuka untuk mengetahui beberapa pola dari data ini. Breast Cancer Data Set Attribute Information: \u00b6 Sample code number: id number Clump Thickness: 1 \u2013 10 Uniformity of Cell Size: 1 \u2013 10 Uniformity of Cell Shape: 1 \u2013 10 Marginal Adhesion: 1 \u2013 10 Single Epithelial Cell Size: 1 \u2013 10 Bare Nuclei: 1 \u2013 10 Bland Chromatin: 1 \u2013 10 Normal Nucleoli: 1 \u2013 10 Mitoses: 1 \u2013 10 Class: (2 for benign, 4 for malignant) Pernyataan masalah: \u00b6 Untuk memodelkan pengklasifikasi knn menggunakan Breast Cancer Dataset untuk memprediksi apakah pasien menderita tumor jinak atau tumor ganas. Model KNN untuk deteksi tumor kanker: \u00b6 Untuk mendiagnosis Kanker Payudara, dokter menggunakan pengalamannya dengan menganalisis rincian yang diberikan oleh Riwayat Medis Masa Lalu Pasien Laporan semua tes dilakukan. Menggunakan classifier KNN yang dimodelkan, kami akan memecahkan masalah dengan cara yang mirip dengan prosedur yang digunakan oleh dokter. Classifier KNN yang dimodelkan akan membandingkan laporan uji pasien baru, metrik observasi dengan catatan pasien (data pelatihan) yang diklasifikasikan dengan benar sebagai jinak atau ganas. K-nearest neighbor classifier implementation with scikit-learn \u00b6 Libraries : Bagian ini melibatkan mengimpor semua library. saya mengimpor numpy dan sklearn, train_test_split, KneighborsClassifier & akurasi_score. import numpy as np from sklearn.preprocessing import Imputer from sklearn.cross_validation import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score saya menggunakan Breast Cancer Dataset. Anda dapat mengunduhnya dari situs web archive.ics.uci.edu. Untuk mengimpor data dan memanipulasinya, saya menggunakan array numpy. Menggunakan metode genfromtxt () , kami mengimpor dataset kami ke dalam array 2d numpy. Anda dapat mengimpor file teks menggunakan fungsi ini. saya menggunakan 3 parameter: fname Ini menangani nama file dengan ekstensi. delimiter String yang digunakan untuk memisahkan nilai. Dalam dataset kami \u201c,\u201d (koma) adalah pemisah. **dtype **Ini menangani tipe data variabel. Semua nilai bersifat numerik dalam basis data saya. Tetapi beberapa nilai hilang dan digantikan oleh \"?\". Jadi, harus melakukan imputasi data. Karena alasan ini, saya menggunakan tipe float. cancer_data = np . genfromtxt ( fname = 'breast-cancer-wisconsin.data' , delimiter = ',' , dtype = float ) Dengan menggunakan kode di atas, kami telah mengimpor data kami ke array 2d numpy. len () : Berfungsi untuk mencari tahu no. catatan dalam data kami. str () : Berfungsi untuk mendapatkan ide tentang struktur dasar data. shape() : Untuk mendapatkan dimensi array. print \"Dataset Lenght:: \" , len ( cancer_data ) print \"Dataset:: \" , str ( cancer_data ) print \"Dataset Shape:: \" , cancer_data . shape output: Dataset Length :: 699 Dataset :: [[ 1.00002500e+06 5.00000000e+00 1.00000000e+00 ... , 1.00000000e+00 1.00000000e+00 2.00000000e+00 ] [ 1.00294500e+06 5.00000000e+00 4.00000000e+00 ... , 2.00000000e+00 1.00000000e+00 2.00000000e+00 ] [ 1.01542500e+06 3.00000000e+00 1.00000000e+00 ... , 1.00000000e+00 1.00000000e+00 2.00000000e+00 ] ... , [ 8.88820000e+05 5.00000000e+00 1.00000000e+01 ... , 1.00000000e+01 2.00000000e+00 4.00000000e+00 ] [ 8.97471000e+05 4.00000000e+00 8.00000000e+00 ... , 6.00000000e+00 1.00000000e+00 4.00000000e+00 ] [ 8.97471000e+05 4.00000000e+00 8.00000000e+00 ... , 4.00000000e+00 1.00000000e+00 4.00000000e+00 ]] Dataset Shape :: ( 699L , 11L ) Kolom dataset pertama kanker terdiri dari id pasien. Untuk membuat proses prediksi ini tidak bias, kita harus menghapus id pasien ini. Kita dapat menggunakan metode numpy delete () untuk operasi ini. delete (): Ini mengembalikan array yang diubah baru. Tiga parameter harus dilewati. arr : Ini memegang nama array. obj : Ini menunjukkan sub-array mana yang akan dihapus. axis : Sumbu tempat untuk menghapus. sumbu = 1 digunakan untuk kolom & sumbu = 0 untuk baris. cancer_data = np . delete ( arr = cancer_data , obj = 0 , axis = 1 ) Sekarang, kami ingin membagi dataset menjadi dataset fitur & label. yaitu, data fitur adalah variabel prediktor, mereka akan membantu kami untuk memprediksi label (variabel kriteria). Di sini, 9 kolom pertama termasuk variabel kontinu yang akan membantu kita untuk memprediksi apakah seorang pasien mengalami tumor jinak atau tumor ganas. X = cancer_data [:, range ( 0 , 9 )] Y = cancer_data [:, 9 ] Data Imputation: \u00b6 Imputasi adalah proses penggantian nilai yang hilang dengan nilai yang diganti. Dalam dataset kami, beberapa kolom memiliki nilai yang hilang. Kita dapat mengganti nilai yang hilang dengan nilai tengah, median, mode atau nilai tertentu apa pun. Sklearn menyediakan metode Imputer () untuk melakukan imputasi dalam 1 baris kode. Kita hanya perlu mendefinisikan missing_values, sumbu, dan strategi. Kami menggunakan nilai \"median\" kolom untuk menggantikan dengan nilai yang hilang. imp = Imputer (missing_values = \"NaN\", strategy = 'median', axis = 0) X = imp.fit_transform (X) imp = Imputer ( missing_values = \"NaN\" , strategy = 'median' , axis = 0 ) X = imp . fit_transform ( X ) Train, Test data split: \u00b6 Untuk membagi data menjadi data train& data uji. Kami menggunakan metode train_test_split () oleh sklearn. train_test_split () : Kami menggunakan 4 parameter X, Y, test_size, random_state X, Y : X adalah array numpy yang terdiri dari dataset fitur & Y berisi label untuk setiap record. test_size : Ini mewakili ukuran data uji yang perlu dipisah. Jika kita menggunakan 0,4, itu menunjukkan 40% data harus dipisahkan dan disimpan sebagai data pengujian. random_state : Keadaan pseudo-random number generator yang digunakan untuk pengambilan sampel acak. Jika Anda ingin mereplikasi hasil kami, maka gunakan nilai random_state yang sama. Sekarang, X_train & y_train adalah dataset training. X_test & y_test sedang menguji dataset. y_train & y_test adalah array numpy 2d dengan 1 kolom. Untuk mengubahnya menjadi array 1d, saya menggunakan ravel (). X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 100 ) y_train = y_train . ravel () y_test = y_test . ravel () Implementasi k-nn : \u00b6 Sekarang saya menyesuaikan algoritma KNN pada data train, memprediksi label untuk dataset dan mencetak akurasi model untuk nilai K yang berbeda (mulai dari 1 hingga 25). Jika array data fitur dimasukkan sebagai parameter, maka array label diberikan sebagai output Accuracy score: Accuracy_score(): Fungsi ini digunakan untuk mencetak akurasi algoritma KNN. Secara akurat, maksud kami adalah rasio titik data yang diprediksi dengan benar dengan semua titik data yang diprediksi. Akurasi sebagai metrik membantu memahami efektivitas algoritme kami. Dibutuhkan 4 parameter. y_true, y_pred, normalize, sample_weight. Dari 4 ini, normalisasi & sample_weight adalah parameter opsional. Parameter y_true menerima larik label yang benar dan y_pred mengambil larik label yang diprediksi yang dikembalikan oleh classifier. Ini mengembalikan akurasi sebagai nilai float. for K in range ( 25 ): K_value = K + 1 neigh = KNeighborsClassifier ( n_neighbors = K_value , weights = 'uniform' , algorithm = 'auto' ) neigh . fit ( X_train , y_train ) y_pred = neigh . predict ( X_test ) print \"Accuracy is \" , accuracy_score ( y_test , y_pred ) * 100 , \" % f or K-Value:\" , K_value Output : Accuracy is 95.2380952381 % for K - Value : 1 Accuracy is 93.3333333333 % for K - Value : 2 Accuracy is 95.7142857143 % for K - Value : 3 Accuracy is 95.2380952381 % for K - Value : 4 Accuracy is 95.7142857143 % for K - Value : 5 Accuracy is 94.7619047619 % for K - Value : 6 Accuracy is 94.7619047619 % for K - Value : 7 Accuracy is 94.2857142857 % for K - Value : 8 Accuracy is 94.7619047619 % for K - Value : 9 Accuracy is 94.2857142857 % for K - Value : 10 Accuracy is 94.2857142857 % for K - Value : 11 Accuracy is 94.7619047619 % for K - Value : 12 Accuracy is 94.7619047619 % for K - Value : 13 Accuracy is 93.8095238095 % for K - Value : 14 Accuracy is 93.8095238095 % for K - Value : 15 Accuracy is 93.8095238095 % for K - Value : 16 Accuracy is 93.8095238095 % for K - Value : 17 Accuracy is 93.8095238095 % for K - Value : 18 Accuracy is 93.8095238095 % for K - Value : 19 Accuracy is 93.8095238095 % for K - Value : 20 Accuracy is 93.8095238095 % for K - Value : 21 Accuracy is 93.8095238095 % for K - Value : 22 Accuracy is 93.8095238095 % for K - Value : 23 Accuracy is 93.8095238095 % for K - Value : 24 Accuracy is 93.8095238095 % for K - Value : 25 K value Vs Accuracy Change Graph \u00b6 Ini menunjukkan bahwa kita mendapatkan akurasi 95,71% pada K = 3, 5. Memilih nilai besar K akan menyebabkan jumlah waktu eksekusi yang lebih besar & kekurangan. Memilih nilai kecil K akan menyebabkan overfitting. Tidak ada cara yang dijamin untuk menemukan nilai terbaik dari K. Jadi, untuk menjalankannya dengan cepat, kami mempertimbangkan K = 3 untuk tutorial ini. my instagram : @linajosalinka","title":"KNN"},{"location":"#knn-sklearn-k-nearest-neighbor-menggunakan-scikit-learn-dengan-breast-cancer-data-set","text":"Implementasi knn classifier di scikit belajar Dalam pengantar k tetangga terdekat dan penerapan pengklasifikasi knn di Python dari awal, Kami membahas aspek-aspek kunci dari algoritma knn dan mengimplementasikan algoritma knn dengan cara yang mudah untuk beberapa dataset pengamatan. Namun dalam implementasiK-Neighbor di scikit learn, kita akan memeriksa Dataset Cancer Breast menggunakan library python sklearn untuk memodelkan algoritma k-nn. Setelah memodelkan knn classifier, kita akan menggunakan model knn tranining untuk memprediksi apakah pasien menderita tumor jinak atau tumor ganas. Kehebatan menggunakan Sklearn adalah memberikan kita fungsionalitas untuk mengimplementasikan algoritma machine learning dalam beberapa baris kode. Ketika kita membahas prinsip di balik algoritma KNN classifier (K-Nearest Neighbor) adalah untuk menemukan jumlah training sampel K yang telah ditentukan terdekat dalam jarak ke titik baru & memprediksi label dari ini. Ukuran jarak umumnya dianggap sebagai jarak Euclidean. Euclidean Distance adalah ukuran jarak yang paling umum digunakan. Jarak Euclidean juga disebut sebagai jarak semata. Penggunaan ukuran jarak Euclidean sangat dianjurkan saat data padat atau kontinu. Jarak Euclidean adalah ukuran kedekatan terbaik. Jarak Euclidean antara dua titik adalah panjang jalur yang menghubungkannya. Teorema Pythagoras memberikan jarak ini antara dua titik.","title":"KNN SKLEARN, K-NEAREST NEIGHBOR MENGGUNAKAN SCIKIT LEARN DENGAN BREAST CANCER DATA SET"},{"location":"#implementasi-knn-dengan-sklearn","text":"","title":"Implementasi knn dengan Sklearn"},{"location":"#wisconsin-breast-cancer-dataset","text":"Wisconsin Breast Cancer Dataset dikumpulkan oleh Dr. William H. Wolberg (dokter), Rumah Sakit Universitas Wisconsin, AS. Dataset ini terdiri dari 10 atribut kontinu dan 1 atribut kelas target. Atribut Class menunjukkan hasil pengamatan, apakah pasien menderita tumor jinak atau tumor ganas. Tumor jinak tidak menyebar ke bagian lain sementara tumor ganas bersifat kanker. Dataset dikumpulkan & didistribusikan secara terbuka untuk mengetahui beberapa pola dari data ini. Atribut Class menunjukkan hasil pengamatan, apakah pasien menderita tumor jinak atau tumor ganas. Tumor jinak tidak menyebar ke bagian lain sementara tumor ganas bersifat kanker. Dataset dikumpulkan & didistribusikan secara terbuka untuk mengetahui beberapa pola dari data ini.","title":"Wisconsin Breast Cancer Dataset"},{"location":"#breast-cancer-data-set-attribute-information","text":"Sample code number: id number Clump Thickness: 1 \u2013 10 Uniformity of Cell Size: 1 \u2013 10 Uniformity of Cell Shape: 1 \u2013 10 Marginal Adhesion: 1 \u2013 10 Single Epithelial Cell Size: 1 \u2013 10 Bare Nuclei: 1 \u2013 10 Bland Chromatin: 1 \u2013 10 Normal Nucleoli: 1 \u2013 10 Mitoses: 1 \u2013 10 Class: (2 for benign, 4 for malignant)","title":"Breast Cancer Data Set Attribute Information:"},{"location":"#pernyataan-masalah","text":"Untuk memodelkan pengklasifikasi knn menggunakan Breast Cancer Dataset untuk memprediksi apakah pasien menderita tumor jinak atau tumor ganas.","title":"Pernyataan masalah:"},{"location":"#model-knn-untuk-deteksi-tumor-kanker","text":"Untuk mendiagnosis Kanker Payudara, dokter menggunakan pengalamannya dengan menganalisis rincian yang diberikan oleh Riwayat Medis Masa Lalu Pasien Laporan semua tes dilakukan. Menggunakan classifier KNN yang dimodelkan, kami akan memecahkan masalah dengan cara yang mirip dengan prosedur yang digunakan oleh dokter. Classifier KNN yang dimodelkan akan membandingkan laporan uji pasien baru, metrik observasi dengan catatan pasien (data pelatihan) yang diklasifikasikan dengan benar sebagai jinak atau ganas.","title":"Model KNN untuk deteksi tumor kanker:"},{"location":"#k-nearest-neighbor-classifier-implementation-with-scikit-learn","text":"Libraries : Bagian ini melibatkan mengimpor semua library. saya mengimpor numpy dan sklearn, train_test_split, KneighborsClassifier & akurasi_score. import numpy as np from sklearn.preprocessing import Imputer from sklearn.cross_validation import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score saya menggunakan Breast Cancer Dataset. Anda dapat mengunduhnya dari situs web archive.ics.uci.edu. Untuk mengimpor data dan memanipulasinya, saya menggunakan array numpy. Menggunakan metode genfromtxt () , kami mengimpor dataset kami ke dalam array 2d numpy. Anda dapat mengimpor file teks menggunakan fungsi ini. saya menggunakan 3 parameter: fname Ini menangani nama file dengan ekstensi. delimiter String yang digunakan untuk memisahkan nilai. Dalam dataset kami \u201c,\u201d (koma) adalah pemisah. **dtype **Ini menangani tipe data variabel. Semua nilai bersifat numerik dalam basis data saya. Tetapi beberapa nilai hilang dan digantikan oleh \"?\". Jadi, harus melakukan imputasi data. Karena alasan ini, saya menggunakan tipe float. cancer_data = np . genfromtxt ( fname = 'breast-cancer-wisconsin.data' , delimiter = ',' , dtype = float ) Dengan menggunakan kode di atas, kami telah mengimpor data kami ke array 2d numpy. len () : Berfungsi untuk mencari tahu no. catatan dalam data kami. str () : Berfungsi untuk mendapatkan ide tentang struktur dasar data. shape() : Untuk mendapatkan dimensi array. print \"Dataset Lenght:: \" , len ( cancer_data ) print \"Dataset:: \" , str ( cancer_data ) print \"Dataset Shape:: \" , cancer_data . shape output: Dataset Length :: 699 Dataset :: [[ 1.00002500e+06 5.00000000e+00 1.00000000e+00 ... , 1.00000000e+00 1.00000000e+00 2.00000000e+00 ] [ 1.00294500e+06 5.00000000e+00 4.00000000e+00 ... , 2.00000000e+00 1.00000000e+00 2.00000000e+00 ] [ 1.01542500e+06 3.00000000e+00 1.00000000e+00 ... , 1.00000000e+00 1.00000000e+00 2.00000000e+00 ] ... , [ 8.88820000e+05 5.00000000e+00 1.00000000e+01 ... , 1.00000000e+01 2.00000000e+00 4.00000000e+00 ] [ 8.97471000e+05 4.00000000e+00 8.00000000e+00 ... , 6.00000000e+00 1.00000000e+00 4.00000000e+00 ] [ 8.97471000e+05 4.00000000e+00 8.00000000e+00 ... , 4.00000000e+00 1.00000000e+00 4.00000000e+00 ]] Dataset Shape :: ( 699L , 11L ) Kolom dataset pertama kanker terdiri dari id pasien. Untuk membuat proses prediksi ini tidak bias, kita harus menghapus id pasien ini. Kita dapat menggunakan metode numpy delete () untuk operasi ini. delete (): Ini mengembalikan array yang diubah baru. Tiga parameter harus dilewati. arr : Ini memegang nama array. obj : Ini menunjukkan sub-array mana yang akan dihapus. axis : Sumbu tempat untuk menghapus. sumbu = 1 digunakan untuk kolom & sumbu = 0 untuk baris. cancer_data = np . delete ( arr = cancer_data , obj = 0 , axis = 1 ) Sekarang, kami ingin membagi dataset menjadi dataset fitur & label. yaitu, data fitur adalah variabel prediktor, mereka akan membantu kami untuk memprediksi label (variabel kriteria). Di sini, 9 kolom pertama termasuk variabel kontinu yang akan membantu kita untuk memprediksi apakah seorang pasien mengalami tumor jinak atau tumor ganas. X = cancer_data [:, range ( 0 , 9 )] Y = cancer_data [:, 9 ]","title":"K-nearest neighbor classifier implementation with scikit-learn"},{"location":"#data-imputation","text":"Imputasi adalah proses penggantian nilai yang hilang dengan nilai yang diganti. Dalam dataset kami, beberapa kolom memiliki nilai yang hilang. Kita dapat mengganti nilai yang hilang dengan nilai tengah, median, mode atau nilai tertentu apa pun. Sklearn menyediakan metode Imputer () untuk melakukan imputasi dalam 1 baris kode. Kita hanya perlu mendefinisikan missing_values, sumbu, dan strategi. Kami menggunakan nilai \"median\" kolom untuk menggantikan dengan nilai yang hilang. imp = Imputer (missing_values = \"NaN\", strategy = 'median', axis = 0) X = imp.fit_transform (X) imp = Imputer ( missing_values = \"NaN\" , strategy = 'median' , axis = 0 ) X = imp . fit_transform ( X )","title":"Data Imputation:"},{"location":"#train-test-data-split","text":"Untuk membagi data menjadi data train& data uji. Kami menggunakan metode train_test_split () oleh sklearn. train_test_split () : Kami menggunakan 4 parameter X, Y, test_size, random_state X, Y : X adalah array numpy yang terdiri dari dataset fitur & Y berisi label untuk setiap record. test_size : Ini mewakili ukuran data uji yang perlu dipisah. Jika kita menggunakan 0,4, itu menunjukkan 40% data harus dipisahkan dan disimpan sebagai data pengujian. random_state : Keadaan pseudo-random number generator yang digunakan untuk pengambilan sampel acak. Jika Anda ingin mereplikasi hasil kami, maka gunakan nilai random_state yang sama. Sekarang, X_train & y_train adalah dataset training. X_test & y_test sedang menguji dataset. y_train & y_test adalah array numpy 2d dengan 1 kolom. Untuk mengubahnya menjadi array 1d, saya menggunakan ravel (). X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 100 ) y_train = y_train . ravel () y_test = y_test . ravel ()","title":"Train, Test data split:"},{"location":"#implementasi-k-nn","text":"Sekarang saya menyesuaikan algoritma KNN pada data train, memprediksi label untuk dataset dan mencetak akurasi model untuk nilai K yang berbeda (mulai dari 1 hingga 25). Jika array data fitur dimasukkan sebagai parameter, maka array label diberikan sebagai output Accuracy score: Accuracy_score(): Fungsi ini digunakan untuk mencetak akurasi algoritma KNN. Secara akurat, maksud kami adalah rasio titik data yang diprediksi dengan benar dengan semua titik data yang diprediksi. Akurasi sebagai metrik membantu memahami efektivitas algoritme kami. Dibutuhkan 4 parameter. y_true, y_pred, normalize, sample_weight. Dari 4 ini, normalisasi & sample_weight adalah parameter opsional. Parameter y_true menerima larik label yang benar dan y_pred mengambil larik label yang diprediksi yang dikembalikan oleh classifier. Ini mengembalikan akurasi sebagai nilai float. for K in range ( 25 ): K_value = K + 1 neigh = KNeighborsClassifier ( n_neighbors = K_value , weights = 'uniform' , algorithm = 'auto' ) neigh . fit ( X_train , y_train ) y_pred = neigh . predict ( X_test ) print \"Accuracy is \" , accuracy_score ( y_test , y_pred ) * 100 , \" % f or K-Value:\" , K_value Output : Accuracy is 95.2380952381 % for K - Value : 1 Accuracy is 93.3333333333 % for K - Value : 2 Accuracy is 95.7142857143 % for K - Value : 3 Accuracy is 95.2380952381 % for K - Value : 4 Accuracy is 95.7142857143 % for K - Value : 5 Accuracy is 94.7619047619 % for K - Value : 6 Accuracy is 94.7619047619 % for K - Value : 7 Accuracy is 94.2857142857 % for K - Value : 8 Accuracy is 94.7619047619 % for K - Value : 9 Accuracy is 94.2857142857 % for K - Value : 10 Accuracy is 94.2857142857 % for K - Value : 11 Accuracy is 94.7619047619 % for K - Value : 12 Accuracy is 94.7619047619 % for K - Value : 13 Accuracy is 93.8095238095 % for K - Value : 14 Accuracy is 93.8095238095 % for K - Value : 15 Accuracy is 93.8095238095 % for K - Value : 16 Accuracy is 93.8095238095 % for K - Value : 17 Accuracy is 93.8095238095 % for K - Value : 18 Accuracy is 93.8095238095 % for K - Value : 19 Accuracy is 93.8095238095 % for K - Value : 20 Accuracy is 93.8095238095 % for K - Value : 21 Accuracy is 93.8095238095 % for K - Value : 22 Accuracy is 93.8095238095 % for K - Value : 23 Accuracy is 93.8095238095 % for K - Value : 24 Accuracy is 93.8095238095 % for K - Value : 25","title":"Implementasi k-nn :"},{"location":"#k-value-vs-accuracy-change-graph","text":"Ini menunjukkan bahwa kita mendapatkan akurasi 95,71% pada K = 3, 5. Memilih nilai besar K akan menyebabkan jumlah waktu eksekusi yang lebih besar & kekurangan. Memilih nilai kecil K akan menyebabkan overfitting. Tidak ada cara yang dijamin untuk menemukan nilai terbaik dari K. Jadi, untuk menjalankannya dengan cepat, kami mempertimbangkan K = 3 untuk tutorial ini. my instagram : @linajosalinka","title":"K value Vs Accuracy Change Graph"},{"location":"authors-notes/","text":"Author's notes \u00b6 Hi, I'm Martin ( @squidfunk ) \u00b6 I'm a freelance polyglot software engineer and entrepreneur from Cologne, Germany with more than 12 years of experience in full-stack web development and system programming. If you're interested in my projects, please see my CV . Why another theme? \u00b6 Some time ago I wanted to release a project to the open, but it was in need of user documentation. I checked out the available tools and stuck with MkDocs, because it was so simple and easy to use. However, none of the available themes convinced me. I wanted to build something that was usable on all screen sizes from the ground up, something beautiful and practical at the same time. Google's Material Design appeared to be the perfect fit and this something became Material, a Material Design theme for MkDocs.","title":"Author's notes"},{"location":"authors-notes/#authors-notes","text":"","title":"Author's notes"},{"location":"authors-notes/#hi-im-martin-squidfunk","text":"I'm a freelance polyglot software engineer and entrepreneur from Cologne, Germany with more than 12 years of experience in full-stack web development and system programming. If you're interested in my projects, please see my CV .","title":"Hi, I'm Martin (@squidfunk)"},{"location":"authors-notes/#why-another-theme","text":"Some time ago I wanted to release a project to the open, but it was in need of user documentation. I checked out the available tools and stuck with MkDocs, because it was so simple and easy to use. However, none of the available themes convinced me. I wanted to build something that was usable on all screen sizes from the ground up, something beautiful and practical at the same time. Google's Material Design appeared to be the perfect fit and this something became Material, a Material Design theme for MkDocs.","title":"Why another theme?"},{"location":"decision-tree/","text":"Decision Tree Using Scikit Learn \u00b6 Dalam tutorial ini, pelajari Decision Tree Classification, langkah-langkah pemilihan atribut, dan bagaimana membangun dan mengoptimalkan Decision Tree Classifier menggunakan paket Python Scikit-learn. Sebagai manajer pemasaran, Anda menginginkan sekumpulan pelanggan yang kemungkinan besar akan membeli produk Anda. Ini adalah bagaimana Anda dapat menghemat anggaran pemasaran Anda dengan menemukan audiens Anda. Sebagai manajer pinjaman, Anda perlu mengidentifikasi aplikasi pinjaman berisiko untuk mencapai tingkat gagal bayar pinjaman yang lebih rendah. Proses mengklasifikasikan pelanggan ke dalam kelompok pelanggan potensial dan non-potensial atau aplikasi pinjaman yang aman atau berisiko dikenal sebagai masalah klasifikasi. Klasifikasi adalah proses dua langkah, langkah belajar dan langkah prediksi. Pada langkah pembelajaran, model dikembangkan berdasarkan data pelatihan yang diberikan. Pada langkah prediksi, model digunakan untuk memprediksi respons untuk data yang diberikan. Decision Tree adalah salah satu algoritma klasifikasi termudah dan populer untuk dipahami dan ditafsirkan. Ini dapat digunakan untuk masalah klasifikasi dan regresi. Algoritma Decision Tree \u00b6 Pohon keputusan adalah struktur pohon seperti bagan di mana simpul internal mewakili fitur (atau atribut), cabang mewakili aturan keputusan, dan setiap simpul daun mewakili hasilnya. Node paling atas dalam pohon keputusan dikenal sebagai simpul akar. Ia belajar mempartisi berdasarkan nilai atribut. Ini partisi pohon dengan cara rekursif panggilan partisi rekursif. Struktur seperti bagan alur ini membantu Anda dalam pengambilan keputusan. Ini visualisasi seperti diagram alur yang dengan mudah meniru pemikiran tingkat manusia. Itulah sebabnya pohon keputusan mudah dipahami dan ditafsirkan. Pohon Keputusan adalah jenis kotak putih dari algoritma ML. Ini berbagi logika pengambilan keputusan internal, yang tidak tersedia dalam jenis algoritma kotak hitam seperti Neural Network. Waktu pelatihannya lebih cepat dibandingkan dengan algoritma jaringan saraf. Kompleksitas waktu pohon keputusan adalah fungsi dari jumlah catatan dan jumlah atribut dalam data yang diberikan. Pohon keputusan adalah metode distribusi-bebas atau non-parametrik, yang tidak bergantung pada asumsi distribusi probabilitas. Pohon keputusan dapat menangani data dimensi tinggi dengan akurasi yang baik. Pengukuran Pilihan Atribut Ukuran pemilihan atribut adalah heuristik untuk memilih kriteria pemisahan yang membagi data menjadi cara terbaik. Ia juga dikenal sebagai aturan pemisahan karena membantu kita untuk menentukan breakpoints untuk tuple pada node yang diberikan. ASM memberikan peringkat untuk setiap fitur (atau atribut) dengan menjelaskan dataset yang diberikan. Atribut skor terbaik akan dipilih sebagai atribut pemisahan (Sumber). Dalam kasus atribut bernilai kontinu, titik perpecahan untuk cabang juga perlu ditentukan. Langkah-langkah seleksi yang paling populer adalah Information Gain, Gain Ratio, dan Gini Index. Information Gain Shannon menemukan konsep entropi, yang mengukur ketidakmurnian set input. Dalam fisika dan matematika, entropi disebut sebagai keacakan atau ketidakmurnian dalam sistem. Dalam teori informasi, ini mengacu pada ketidakmurnian dalam sekelompok contoh. Keuntungan informasi adalah berkurangnya entropi. Informasi gain menghitung perbedaan antara entropi sebelum split dan rata-rata entropi setelah split dari dataset berdasarkan nilai atribut yang diberikan. Algoritma decision tree ID3 (Iterative Dichotomiser) menggunakan informasi gain. Membuat Decision Tree dengan Scikit-learn \u00b6 Impor Library yang diperlukan \u00b6 pertama mari load library yang diperlukan # Load libraries import pandas as pd from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier from sklearn.model_selection import train_test_split # Import train_test_split function from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation Memuat Data \u00b6 Pertama mari kita memuat dataset Pima Indian Diabetes yang dibutuhkan menggunakan fungsi baca CSV panda. Anda dapat mengunduh data di sini. col_names = [ 'pregnant' , 'glucose' , 'bp' , 'skin' , 'insulin' , 'bmi' , 'pedigree' , 'age' , 'label' ] # load dataset pima = pd . read_csv ( \"pima-indians-diabetes.csv\" ) pima . columns = col_names pima . head () Pemilihan Fitur \u00b6 Di sini, Anda perlu membagi kolom yang diberikan menjadi dua jenis variabel dependen (atau variabel target) dan variabel independen (atau variabel fitur). #split dataset in features and target variable feature_cols = [ 'pregnant' , 'insulin' , 'bmi' , 'age' , 'glucose' , 'bp' , 'pedigree' ] X = pima [ feature_cols ] # Features y = pima . label # Target variable Memisahkan Data \u00b6 Untuk memahami kinerja model, membagi dataset ke dalam set pelatihan dan set tes adalah strategi yang baik. Mari kita pisahkan dataset dengan menggunakan function train_test_split (). Anda harus melewati 3 parameter fitur, target, dan ukuran test_set. # Split dataset into training set and test set X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) # 70% training and 30% test Membuat model Decision Tree Mari kita buat model decision tree menggunakan scikit learn # Create Decision Tree classifer object clf = DecisionTreeClassifier () # Train Decision Tree Classifer clf = clf . fit ( X_train , y_train ) #Predict the response for test dataset y_pred = clf . predict ( X_test ) Mengevaluasi Model \u00b6 Mari kita perkirakan, seberapa akurat classifier atau model dapat memprediksi jenis kultivar. Akurasi dapat dihitung dengan membandingkan nilai set tes aktual dan nilai prediksi. # Model Accuracy, how often is the classifier correct? print ( \"Accuracy:\" , metrics . accuracy_score ( y_test , y_pred )) Accuracy : 0.6753246753246753 Nah, Anda mendapat tingkat klasifikasi 67,53%, dianggap sebagai akurasi yang baik. Anda dapat meningkatkan akurasi ini dengan menyetel parameter dalam Decision Tree Algorithm. Memvisualisasikan decision tree \u00b6 Anda dapat menggunakan fungsi export_graphviz dari Scikit-learn untuk menampilkan pohon dalam notebook Jupyter. Untuk merencanakan pohon, Anda juga perlu menginstal graphviz dan pydotplus. pip install graphviz pip install pydotplus Fungsi export_graphviz mengubah classifier pohon keputusan menjadi file dot dan pydotplus mengubah file dot ini menjadi png atau bentuk yang dapat ditampilkan di Jupyter. from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image import pydotplus dot_data = StringIO () export_graphviz ( clf , out_file = dot_data , filled = True , rounded = True , special_characters = True , feature_names = feature_cols , class_names = [ '0' , '1' ]) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) graph . write_png ( 'diabetes.png' ) Image ( graph . create_png ())","title":"Decision Tree"},{"location":"decision-tree/#decision-tree-using-scikit-learn","text":"Dalam tutorial ini, pelajari Decision Tree Classification, langkah-langkah pemilihan atribut, dan bagaimana membangun dan mengoptimalkan Decision Tree Classifier menggunakan paket Python Scikit-learn. Sebagai manajer pemasaran, Anda menginginkan sekumpulan pelanggan yang kemungkinan besar akan membeli produk Anda. Ini adalah bagaimana Anda dapat menghemat anggaran pemasaran Anda dengan menemukan audiens Anda. Sebagai manajer pinjaman, Anda perlu mengidentifikasi aplikasi pinjaman berisiko untuk mencapai tingkat gagal bayar pinjaman yang lebih rendah. Proses mengklasifikasikan pelanggan ke dalam kelompok pelanggan potensial dan non-potensial atau aplikasi pinjaman yang aman atau berisiko dikenal sebagai masalah klasifikasi. Klasifikasi adalah proses dua langkah, langkah belajar dan langkah prediksi. Pada langkah pembelajaran, model dikembangkan berdasarkan data pelatihan yang diberikan. Pada langkah prediksi, model digunakan untuk memprediksi respons untuk data yang diberikan. Decision Tree adalah salah satu algoritma klasifikasi termudah dan populer untuk dipahami dan ditafsirkan. Ini dapat digunakan untuk masalah klasifikasi dan regresi.","title":"Decision Tree Using Scikit Learn"},{"location":"decision-tree/#algoritma-decision-tree","text":"Pohon keputusan adalah struktur pohon seperti bagan di mana simpul internal mewakili fitur (atau atribut), cabang mewakili aturan keputusan, dan setiap simpul daun mewakili hasilnya. Node paling atas dalam pohon keputusan dikenal sebagai simpul akar. Ia belajar mempartisi berdasarkan nilai atribut. Ini partisi pohon dengan cara rekursif panggilan partisi rekursif. Struktur seperti bagan alur ini membantu Anda dalam pengambilan keputusan. Ini visualisasi seperti diagram alur yang dengan mudah meniru pemikiran tingkat manusia. Itulah sebabnya pohon keputusan mudah dipahami dan ditafsirkan. Pohon Keputusan adalah jenis kotak putih dari algoritma ML. Ini berbagi logika pengambilan keputusan internal, yang tidak tersedia dalam jenis algoritma kotak hitam seperti Neural Network. Waktu pelatihannya lebih cepat dibandingkan dengan algoritma jaringan saraf. Kompleksitas waktu pohon keputusan adalah fungsi dari jumlah catatan dan jumlah atribut dalam data yang diberikan. Pohon keputusan adalah metode distribusi-bebas atau non-parametrik, yang tidak bergantung pada asumsi distribusi probabilitas. Pohon keputusan dapat menangani data dimensi tinggi dengan akurasi yang baik. Pengukuran Pilihan Atribut Ukuran pemilihan atribut adalah heuristik untuk memilih kriteria pemisahan yang membagi data menjadi cara terbaik. Ia juga dikenal sebagai aturan pemisahan karena membantu kita untuk menentukan breakpoints untuk tuple pada node yang diberikan. ASM memberikan peringkat untuk setiap fitur (atau atribut) dengan menjelaskan dataset yang diberikan. Atribut skor terbaik akan dipilih sebagai atribut pemisahan (Sumber). Dalam kasus atribut bernilai kontinu, titik perpecahan untuk cabang juga perlu ditentukan. Langkah-langkah seleksi yang paling populer adalah Information Gain, Gain Ratio, dan Gini Index. Information Gain Shannon menemukan konsep entropi, yang mengukur ketidakmurnian set input. Dalam fisika dan matematika, entropi disebut sebagai keacakan atau ketidakmurnian dalam sistem. Dalam teori informasi, ini mengacu pada ketidakmurnian dalam sekelompok contoh. Keuntungan informasi adalah berkurangnya entropi. Informasi gain menghitung perbedaan antara entropi sebelum split dan rata-rata entropi setelah split dari dataset berdasarkan nilai atribut yang diberikan. Algoritma decision tree ID3 (Iterative Dichotomiser) menggunakan informasi gain.","title":"Algoritma Decision Tree"},{"location":"decision-tree/#membuat-decision-tree-dengan-scikit-learn","text":"","title":"Membuat Decision Tree dengan Scikit-learn"},{"location":"decision-tree/#impor-library-yang-diperlukan","text":"pertama mari load library yang diperlukan # Load libraries import pandas as pd from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier from sklearn.model_selection import train_test_split # Import train_test_split function from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation","title":"Impor Library yang diperlukan"},{"location":"decision-tree/#memuat-data","text":"Pertama mari kita memuat dataset Pima Indian Diabetes yang dibutuhkan menggunakan fungsi baca CSV panda. Anda dapat mengunduh data di sini. col_names = [ 'pregnant' , 'glucose' , 'bp' , 'skin' , 'insulin' , 'bmi' , 'pedigree' , 'age' , 'label' ] # load dataset pima = pd . read_csv ( \"pima-indians-diabetes.csv\" ) pima . columns = col_names pima . head ()","title":"Memuat Data"},{"location":"decision-tree/#pemilihan-fitur","text":"Di sini, Anda perlu membagi kolom yang diberikan menjadi dua jenis variabel dependen (atau variabel target) dan variabel independen (atau variabel fitur). #split dataset in features and target variable feature_cols = [ 'pregnant' , 'insulin' , 'bmi' , 'age' , 'glucose' , 'bp' , 'pedigree' ] X = pima [ feature_cols ] # Features y = pima . label # Target variable","title":"Pemilihan Fitur"},{"location":"decision-tree/#memisahkan-data","text":"Untuk memahami kinerja model, membagi dataset ke dalam set pelatihan dan set tes adalah strategi yang baik. Mari kita pisahkan dataset dengan menggunakan function train_test_split (). Anda harus melewati 3 parameter fitur, target, dan ukuran test_set. # Split dataset into training set and test set X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 1 ) # 70% training and 30% test Membuat model Decision Tree Mari kita buat model decision tree menggunakan scikit learn # Create Decision Tree classifer object clf = DecisionTreeClassifier () # Train Decision Tree Classifer clf = clf . fit ( X_train , y_train ) #Predict the response for test dataset y_pred = clf . predict ( X_test )","title":"Memisahkan Data"},{"location":"decision-tree/#mengevaluasi-model","text":"Mari kita perkirakan, seberapa akurat classifier atau model dapat memprediksi jenis kultivar. Akurasi dapat dihitung dengan membandingkan nilai set tes aktual dan nilai prediksi. # Model Accuracy, how often is the classifier correct? print ( \"Accuracy:\" , metrics . accuracy_score ( y_test , y_pred )) Accuracy : 0.6753246753246753 Nah, Anda mendapat tingkat klasifikasi 67,53%, dianggap sebagai akurasi yang baik. Anda dapat meningkatkan akurasi ini dengan menyetel parameter dalam Decision Tree Algorithm.","title":"Mengevaluasi Model"},{"location":"decision-tree/#memvisualisasikan-decision-tree","text":"Anda dapat menggunakan fungsi export_graphviz dari Scikit-learn untuk menampilkan pohon dalam notebook Jupyter. Untuk merencanakan pohon, Anda juga perlu menginstal graphviz dan pydotplus. pip install graphviz pip install pydotplus Fungsi export_graphviz mengubah classifier pohon keputusan menjadi file dot dan pydotplus mengubah file dot ini menjadi png atau bentuk yang dapat ditampilkan di Jupyter. from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image import pydotplus dot_data = StringIO () export_graphviz ( clf , out_file = dot_data , filled = True , rounded = True , special_characters = True , feature_names = feature_cols , class_names = [ '0' , '1' ]) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) graph . write_png ( 'diabetes.png' ) Image ( graph . create_png ())","title":"Memvisualisasikan decision tree"}]}